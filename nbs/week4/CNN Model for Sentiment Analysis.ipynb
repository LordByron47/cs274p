{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutional Neural Network in Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the Week 4 lecture about Recurrent Neural Networks. The dataset we use in this notebook is from this kaggle competition: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell is the code that we looked at in the lecture on how to bulid a bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our tokenizer and counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "# Declare a sentence \n",
    "sent = '''I am taking the MCS class CS274P \n",
    "          and I am learning how to teach a \n",
    "          machine to understand language'''\n",
    "# Lets tokenize our sentence\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sent.lower())\n",
    "bow = Counter(tokens)\n",
    "bow.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data as torch_data\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random.seed(84848)\n",
    "np.random.seed(84848)\n",
    "torch.manual_seed(84848)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Lets download our data from Kaggle\n",
    "Look at one of the previous notebooks on how to setup Kaggle on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip sentiment-analysis-on-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm sentiment-analysis-on-movie-reviews.zip && mkdir ../../data/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *csv && mv *.zip ../../data/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../data/sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/sentiment/train.tsv.zip', sep=\"\\t\")\n",
    "test = pd.read_csv('../../data/sentiment/test.tsv.zip', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets create a custom Dataset object to load and split our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewDataset(torch_data.Dataset):\n",
    "    name = 'movie_reviews'\n",
    "    text_field = torch_data.Field(tokenize = 'spacy', batch_first = True)\n",
    "    label_field = torch_data.LabelField(dtype = torch.float)\n",
    "    path = ''\n",
    "    def __init__(self, data, fields, **kwargs):\n",
    "        examples = []\n",
    "        for idx, row in data.iterrows():\n",
    "            text = row['Phrase']\n",
    "            label = row['Sentiment']\n",
    "            examples.append(torch_data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super(MovieReviewDataset, self).__init__(examples, fields, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.text)\n",
    "    \n",
    "    def split(self, split_ratio=0.7, stratified=False, strata_field='label',\n",
    "              random_state=None):\n",
    "        \"\"\"Create train-test(-valid?) splits from the instance's examples.\n",
    "        Returns:\n",
    "            Tuple[Dataset]: Datasets for train, validation, and\n",
    "            test splits in that order, if the splits are provided.\n",
    "        \"\"\"\n",
    "        train_ratio, test_ratio, val_ratio = self.check_split_ratio(split_ratio)\n",
    "\n",
    "        # For the permutations\n",
    "        rnd = torch_data.utils.RandomShuffler(random_state)\n",
    "        if not stratified:\n",
    "            train_data, test_data, val_data = self.rationed_split(self.examples, train_ratio,\n",
    "                                                             test_ratio, val_ratio, rnd)\n",
    "        else:\n",
    "            if strata_field not in self.fields:\n",
    "                raise ValueError(\"Invalid field name for strata_field {}\"\n",
    "                                 .format(strata_field))\n",
    "            strata = stratify(self.examples, strata_field)\n",
    "            train_data, test_data, val_data = [], [], []\n",
    "            for group in strata:\n",
    "                # Stratify each group and add together the indices.\n",
    "                group_train, group_test, group_val = rationed_split(group, train_ratio,\n",
    "                                                                    test_ratio, val_ratio,\n",
    "                                                                    rnd)\n",
    "                train_data += group_train\n",
    "                test_data += group_test\n",
    "                val_data += group_val\n",
    "\n",
    "        splits = tuple(torch_data.Dataset(d, self.fields)\n",
    "                       for d in (train_data, val_data, test_data) if d)\n",
    "\n",
    "        # In case the parent sort key isn't none\n",
    "        if self.sort_key:\n",
    "            for subset in splits:\n",
    "                subset.sort_key = self.sort_key\n",
    "        return splits\n",
    "    \n",
    "    def check_split_ratio(self, split_ratio):\n",
    "        \"\"\"\"Check that the split ratio argument is not malformed\"\"\"\n",
    "        valid_ratio = 0.\n",
    "        if isinstance(split_ratio, float):\n",
    "            # Only the train set relative ratio is provided\n",
    "            # Assert in bounds, validation size is zero\n",
    "            assert 0. < split_ratio < 1., (\n",
    "                \"Split ratio {} not between 0 and 1\".format(split_ratio))\n",
    "\n",
    "            test_ratio = 1. - split_ratio\n",
    "            return (split_ratio, test_ratio, valid_ratio)\n",
    "        elif isinstance(split_ratio, list):\n",
    "            # A list of relative ratios is provided\n",
    "            length = len(split_ratio)\n",
    "            assert length == 2 or length == 3, (\n",
    "                \"Length of split ratio list should be 2 or 3, got {}\".format(split_ratio))\n",
    "\n",
    "            # Normalize if necessary\n",
    "            ratio_sum = sum(split_ratio)\n",
    "            if not ratio_sum == 1.:\n",
    "                split_ratio = [float(ratio) / ratio_sum for ratio in split_ratio]\n",
    "\n",
    "            if length == 2:\n",
    "                return tuple(split_ratio + [valid_ratio])\n",
    "            return tuple(split_ratio)\n",
    "        else:\n",
    "            raise ValueError('Split ratio must be float or a list, got {}'\n",
    "                             .format(type(split_ratio)))\n",
    "    \n",
    "    def rationed_split(self, examples, train_ratio, test_ratio, val_ratio, rnd):\n",
    "    \n",
    "        N = len(examples)\n",
    "        print(N)\n",
    "        randperm = rnd(range(N))\n",
    "        train_len = int(round(train_ratio * N))\n",
    "\n",
    "        # Due to possible rounding problems\n",
    "        if not val_ratio:\n",
    "            test_len = N - train_len\n",
    "        else:\n",
    "            test_len = int(round(test_ratio * N))\n",
    "\n",
    "        indices = (randperm[:train_len],  # Train\n",
    "                   randperm[train_len:train_len + test_len],  # Test\n",
    "                   randperm[train_len + test_len:])  # Validation\n",
    "\n",
    "        # There's a possibly empty list for the validation set\n",
    "        data = tuple([examples[i] for i in index] for index in indices)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clean and prepare the data for our CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare our text token field and our label field. We also instantiate the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = torch_data.Field(tokenize = 'spacy', batch_first = True)\n",
    "label_field = torch_data.LabelField(dtype = torch.float)\n",
    "dataset = MovieReviewDataset(train, \n",
    "                             fields = [('text', text_field), \n",
    "                             ('label', label_field)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = dataset.split(random_state = random.seed(48484))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build up out word embedding based on the Glove embedding model which we will cover next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(train_data, \n",
    "                 max_size = 25_000, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = torch_data.BucketIterator.splits(\n",
    "                                    (train_data, valid_data), \n",
    "                                    batch_size = 64, \n",
    "                                    device = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Lets declare a basic model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 emb_dim,\n",
    "                 layer_sizes,\n",
    "                 hidden_size,\n",
    "                 out_dim,\n",
    "                 pad_idx,\n",
    "                 device='cpu',\n",
    "                 activation=nn.ReLU):\n",
    "        \n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, emb_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleDict({})\n",
    "        for idx in layer_sizes:\n",
    "            self.convs['conv'+str(idx)] = nn.Conv2d(in_channels = 1, \n",
    "                                                    out_channels = hidden_size, \n",
    "                                                    kernel_size = (idx, emb_dim))\n",
    "        \n",
    "        self.out = nn.Linear(len(layer_sizes) * hidden_size, out_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.activation = activation()\n",
    "        \n",
    "        # Move the model to the gpu if its available\n",
    "        self.to(device)\n",
    "        self.criterion = self.criterion.to(device)\n",
    "        \n",
    "        self.min_loss = float('inf')\n",
    "        \n",
    "        trained_embeddings = text_field.vocab.vectors\n",
    "        self.embed.weight.data.copy_(trained_embeddings)\n",
    "        \n",
    "        unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
    "        self.embed.weight.data[unk_idx] = torch.zeros(em_bed)\n",
    "        self.embed.weight.data[pad_idx] = torch.zeros(em_bed)\n",
    "\n",
    "    def forward(self, in_text):\n",
    "        x = self.embed(in_text)\n",
    "        x = x.unsqueeze(1)\n",
    "        conv = [self.activation(conv(x)).squeeze(3) for _,conv in self.convs.items()]\n",
    "        pool = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in conv]\n",
    "        cat = self.dropout(torch.cat(pool, dim = 1))\n",
    "        return self.out(cat)\n",
    "    \n",
    "    def train_(self, \n",
    "              train_iter, \n",
    "              epochs=5, \n",
    "              model_out_path=None):\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        for e_idx in range(epochs):\n",
    "            train_loss, train_acc = self._train_(train_iter)\n",
    "            #eval_loss, eval_acc = self.evaluate(test_iter)\n",
    "        \n",
    "            #if model_out_path and eval_loss < self.min_loss:\n",
    "            #    self.min_loss = eval_loss\n",
    "            #    self.save(model_out_path)\n",
    "        \n",
    "            print(f'Epoch: {e_idx}')\n",
    "            print(f'\\t Loss: {train_loss:.3f} | Acc: {train_acc*100:.2f}%')\n",
    "            #print(f'\\t Val. Loss: {eval_loss:.3f} |  Val. Acc: {eval_acc*100:.2f}%')\n",
    "    \n",
    "    def _train_(self, iterator):\n",
    "        t_loss = 0\n",
    "        t_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self(batch.text).squeeze(1)\n",
    "            loss = self.criterion(predictions, batch.label)\n",
    "            # Calculate the accuracy\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "            correct = (rounded_preds == batch.label).float() #convert into float for division \n",
    "            acc = correct.sum() / len(correct)\n",
    "        \n",
    "            loss.backward()\n",
    "        \n",
    "            self.optimizer.step()\n",
    "        \n",
    "            t_loss += loss.item()\n",
    "            t_acc += acc.item()\n",
    "        \n",
    "        return t_loss / len(iterator), t_acc / len(iterator)\n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        e_loss = 0\n",
    "        e_acc = 0\n",
    "    \n",
    "        self.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                predictions = self(batch.text).squeeze(1)\n",
    "            \n",
    "                loss = self.criterion(predictions, batch.label)\n",
    "            \n",
    "                rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "                correct = (rounded_preds == batch.label).float() #convert into float for division \n",
    "                acc = correct.sum() / len(correct)\n",
    "\n",
    "                e_loss += loss.item()\n",
    "                e_acc += acc.item()\n",
    "        \n",
    "        return e_loss / len(iterator), e_acc / len(iterator)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we declare our hyper paramters, instantiate our model and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_bed = 100\n",
    "hidden_size = 100\n",
    "layer_size = [3,4,5]\n",
    "output_dim = 1\n",
    "input_size = len(text_field.vocab)\n",
    "pad_idx = text_field.vocab.stoi[text_field.pad_token]\n",
    "\n",
    "model = LanguageModel( \n",
    "                      input_size, \n",
    "                      em_bed,\n",
    "                      layer_size, \n",
    "                      hidden_size, \n",
    "                      output_dim, \n",
    "                      pad_idx,\n",
    "                      device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_(train_iterator 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "1. What is wrong with the model and/or training above. Can you fix it?\n",
    "2. What other hyper parameters would be better here?\n",
    "3. Do predictions on the test set and share your best metrics and how you got there"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py37Torch14",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
